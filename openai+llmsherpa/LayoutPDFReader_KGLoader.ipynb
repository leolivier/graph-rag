{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Downloading neo4j-5.17.0.tar.gz (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pytz in /home/olivi/.local/lib/python3.8/site-packages (from neo4j) (2023.3.post1)\n",
      "Building wheels for collected packages: neo4j\n",
      "  Building wheel for neo4j (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neo4j: filename=neo4j-5.17.0-py3-none-any.whl size=273834 sha256=ad2aa97e9c0ce7a1ebae7b0c5ac94e9c3adfae964fc403616073a77e5d31bbc9\n",
      "  Stored in directory: /home/olivi/.cache/pip/wheels/48/d1/b5/eb3936dab20b64ada48fe1b564fbb7ccfe69d736aebc08f138\n",
      "Successfully built neo4j\n",
      "Installing collected packages: neo4j\n",
      "Successfully installed neo4j-5.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is the demo of:\n",
    "#   - using LayoutPDFReader to read PDF files\n",
    "#   - mapping PDF elements into a property graph\n",
    "#   - saving PDF elements into Neo4j\n",
    "#\n",
    "\n",
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
    "\n",
    "file_location = '/home/olivi/devt/graph-rag/pdfs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "# Please change the following variables to your own Neo4j instance\n",
    "NEO4J_URL = \"neo4j+s://b5533a65.databases.neo4j.io\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"YOQWQzJPEETnYSMLxmXwednBpDNOJXxqt3w8TGiqD6U\"\n",
    "NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "\n",
    "def initialiseNeo4j():\n",
    "    cypher_schema = [\n",
    "        \"CREATE CONSTRAINT sectionKey IF NOT EXISTS FOR (c:Section) REQUIRE (c.key) IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT chunkKey IF NOT EXISTS FOR (c:Chunk) REQUIRE (c.key) IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT documentKey IF NOT EXISTS FOR (c:Document) REQUIRE (c.url_hash) IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT tableKey IF NOT EXISTS FOR (c:Table) REQUIRE (c.key) IS UNIQUE;\",\n",
    "        \"CALL db.index.vector.createNodeIndex('chunkVectorIndex', 'Embedding', 'value', 1536, 'COSINE');\"\n",
    "    ]\n",
    "\n",
    "    driver = GraphDatabase.driver(NEO4J_URL, database=NEO4J_DATABASE, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "    with driver.session() as session:\n",
    "        for cypher in cypher_schema:\n",
    "            session.run(cypher)\n",
    "    driver.close()\n",
    "    \n",
    "\n",
    "def ingestDocumentNeo4j(doc, doc_location):\n",
    "\n",
    "\n",
    "    cypher_pool = [\n",
    "        # 0 - Document\n",
    "        \"MERGE (d:Document {url_hash: $doc_url_hash_val}) ON CREATE SET d.url = $doc_url_val RETURN d;\",  \n",
    "        # 1 - Section\n",
    "        \"MERGE (p:Section {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$title_hash_val}) ON CREATE SET p.page_idx = $page_idx_val, p.title_hash = $title_hash_val, p.block_idx = $block_idx_val, p.title = $title_val, p.tag = $tag_val, p.level = $level_val RETURN p;\",\n",
    "        # 2 - Link Section with the Document\n",
    "        \"MATCH (d:Document {url_hash: $doc_url_hash_val}) MATCH (s:Section {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$title_hash_val}) MERGE (d)<-[:HAS_DOCUMENT]-(s);\",\n",
    "        # 3 - Link Section with a parent section\n",
    "        \"MATCH (s1:Section {key: $doc_url_hash_val+'|'+$parent_block_idx_val+'|'+$parent_title_hash_val}) MATCH (s2:Section {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$title_hash_val}) MERGE (s1)<-[:UNDER_SECTION]-(s2);\",\n",
    "        # 4 - Chunk\n",
    "        \"MERGE (c:Chunk {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$sentences_hash_val}) ON CREATE SET c.sentences = $sentences_val, c.sentences_hash = $sentences_hash_val, c.block_idx = $block_idx_val, c.page_idx = $page_idx_val, c.tag = $tag_val, c.level = $level_val RETURN c;\",\n",
    "        # 5 - Link Chunk to Section\n",
    "        \"MATCH (c:Chunk {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$sentences_hash_val}) MATCH (s:Section {key:$doc_url_hash_val+'|'+$parent_block_idx_val+'|'+$parent_hash_val}) MERGE (s)<-[:HAS_PARENT]-(c);\",\n",
    "        # 6 - Table\n",
    "        \"MERGE (t:Table {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$name_val}) ON CREATE SET t.name = $name_val, t.doc_url_hash = $doc_url_hash_val, t.block_idx = $block_idx_val, t.page_idx = $page_idx_val, t.html = $html_val, t.rows = $rows_val RETURN t;\",\n",
    "        # 7 - Link Table to Section\n",
    "        \"MATCH (t:Table {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$name_val}) MATCH (s:Section {key: $doc_url_hash_val+'|'+$parent_block_idx_val+'|'+$parent_hash_val}) MERGE (s)<-[:HAS_PARENT]-(t);\",\n",
    "        # 8 - Link Table to Document if no parent section\n",
    "        \"MATCH (t:Table {key: $doc_url_hash_val+'|'+$block_idx_val+'|'+$name_val}) MATCH (s:Document {url_hash: $doc_url_hash_val}) MERGE (s)<-[:HAS_PARENT]-(t);\"\n",
    "    ]\n",
    "\n",
    "    driver = GraphDatabase.driver(NEO4J_URL, database=NEO4J_DATABASE, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "    with driver.session() as session:\n",
    "        cypher = \"\"\n",
    "\n",
    "        # 1 - Create Document node\n",
    "        doc_url_val = doc_location\n",
    "        doc_url_hash_val = hashlib.md5(doc_url_val.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        cypher = cypher_pool[0]\n",
    "        session.run(cypher, doc_url_hash_val=doc_url_hash_val, doc_url_val=doc_url_val)\n",
    "\n",
    "        # 2 - Create Section nodes\n",
    "        \n",
    "        countSection = 0\n",
    "        for sec in doc.sections():\n",
    "            sec_title_val = sec.title\n",
    "            sec_title_hash_val = hashlib.md5(sec_title_val.encode(\"utf-8\")).hexdigest()\n",
    "            sec_tag_val = sec.tag\n",
    "            sec_level_val = sec.level\n",
    "            sec_page_idx_val = sec.page_idx\n",
    "            sec_block_idx_val = sec.block_idx\n",
    "\n",
    "            # MERGE section node\n",
    "            if not sec_tag_val == 'table':\n",
    "                cypher = cypher_pool[1]\n",
    "                session.run(cypher, page_idx_val=sec_page_idx_val\n",
    "                                , title_hash_val=sec_title_hash_val\n",
    "                                , title_val=sec_title_val\n",
    "                                , tag_val=sec_tag_val\n",
    "                                , level_val=sec_level_val\n",
    "                                , block_idx_val=sec_block_idx_val\n",
    "                                , doc_url_hash_val=doc_url_hash_val\n",
    "                            )\n",
    "\n",
    "                # Link Section with a parent section or Document\n",
    "\n",
    "                sec_parent_val = str(sec.parent.to_text())\n",
    "\n",
    "                if sec_parent_val == \"None\":    # use Document as parent\n",
    "\n",
    "                    cypher = cypher_pool[2]\n",
    "                    session.run(cypher, page_idx_val=sec_page_idx_val\n",
    "                                    , title_hash_val=sec_title_hash_val\n",
    "                                    , doc_url_hash_val=doc_url_hash_val\n",
    "                                    , block_idx_val=sec_block_idx_val\n",
    "                                )\n",
    "\n",
    "                else:   # use parent section\n",
    "                    sec_parent_title_hash_val = hashlib.md5(sec_parent_val.encode(\"utf-8\")).hexdigest()\n",
    "                    sec_parent_page_idx_val = sec.parent.page_idx\n",
    "                    sec_parent_block_idx_val = sec.parent.block_idx\n",
    "\n",
    "                    cypher = cypher_pool[3]\n",
    "                    session.run(cypher, page_idx_val=sec_page_idx_val\n",
    "                                    , title_hash_val=sec_title_hash_val\n",
    "                                    , block_idx_val=sec_block_idx_val\n",
    "                                    , parent_page_idx_val=sec_parent_page_idx_val\n",
    "                                    , parent_title_hash_val=sec_parent_title_hash_val\n",
    "                                    , parent_block_idx_val=sec_parent_block_idx_val\n",
    "                                    , doc_url_hash_val=doc_url_hash_val\n",
    "                                )\n",
    "            # **** if sec_parent_val == \"None\":    \n",
    "\n",
    "            countSection += 1\n",
    "        # **** for sec in doc.sections():\n",
    "\n",
    "        \n",
    "        # ------- Continue within the blocks -------\n",
    "        # 3 - Create Chunk nodes from chunks\n",
    "            \n",
    "        countChunk = 0\n",
    "        for chk in doc.chunks():\n",
    "\n",
    "            chunk_block_idx_val = chk.block_idx\n",
    "            chunk_page_idx_val = chk.page_idx\n",
    "            chunk_tag_val = chk.tag\n",
    "            chunk_level_val = chk.level\n",
    "            chunk_sentences = \"\\n\".join(chk.sentences)\n",
    "\n",
    "            # MERGE Chunk node\n",
    "            if not chunk_tag_val == 'table':\n",
    "                chunk_sentences_hash_val = hashlib.md5(chunk_sentences.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "                # MERGE chunk node\n",
    "                cypher = cypher_pool[4]\n",
    "                session.run(cypher, sentences_hash_val=chunk_sentences_hash_val\n",
    "                                , sentences_val=chunk_sentences\n",
    "                                , block_idx_val=chunk_block_idx_val\n",
    "                                , page_idx_val=chunk_page_idx_val\n",
    "                                , tag_val=chunk_tag_val\n",
    "                                , level_val=chunk_level_val\n",
    "                                , doc_url_hash_val=doc_url_hash_val\n",
    "                            )\n",
    "            \n",
    "                # Link chunk with a section\n",
    "                # Chunk always has a parent section \n",
    "\n",
    "                chk_parent_val = str(chk.parent.to_text())\n",
    "                \n",
    "                if not chk_parent_val == \"None\":\n",
    "                    chk_parent_hash_val = hashlib.md5(chk_parent_val.encode(\"utf-8\")).hexdigest()\n",
    "                    chk_parent_page_idx_val = chk.parent.page_idx\n",
    "                    chk_parent_block_idx_val = chk.parent.block_idx\n",
    "\n",
    "                    cypher = cypher_pool[5]\n",
    "                    session.run(cypher, sentences_hash_val=chunk_sentences_hash_val\n",
    "                                    , block_idx_val=chunk_block_idx_val\n",
    "                                    , parent_hash_val=chk_parent_hash_val\n",
    "                                    , parent_block_idx_val=chk_parent_block_idx_val\n",
    "                                    , doc_url_hash_val=doc_url_hash_val\n",
    "                                )\n",
    "                    \n",
    "                # Link sentence \n",
    "                #   >> TO DO for smaller token length\n",
    "\n",
    "                countChunk += 1\n",
    "        # **** for chk in doc.chunks(): \n",
    "\n",
    "        # 4 - Create Table nodes\n",
    "\n",
    "        countTable = 0\n",
    "        for tb in doc.tables():\n",
    "            page_idx_val = tb.page_idx\n",
    "            block_idx_val = tb.block_idx\n",
    "            name_val = 'block#' + str(block_idx_val) + '_' + tb.name\n",
    "            html_val = tb.to_html()\n",
    "            rows_val = len(tb.rows)\n",
    "\n",
    "            # MERGE table node\n",
    "\n",
    "            cypher = cypher_pool[6]\n",
    "            session.run(cypher, block_idx_val=block_idx_val\n",
    "                            , page_idx_val=page_idx_val\n",
    "                            , name_val=name_val\n",
    "                            , html_val=html_val\n",
    "                            , rows_val=rows_val\n",
    "                            , doc_url_hash_val=doc_url_hash_val\n",
    "                        )\n",
    "            \n",
    "            # Link table with a section\n",
    "            # Table always has a parent section \n",
    "\n",
    "            table_parent_val = str(tb.parent.to_text())\n",
    "            \n",
    "            if not table_parent_val == \"None\":\n",
    "                table_parent_hash_val = hashlib.md5(table_parent_val.encode(\"utf-8\")).hexdigest()\n",
    "                table_parent_page_idx_val = tb.parent.page_idx\n",
    "                table_parent_block_idx_val = tb.parent.block_idx\n",
    "\n",
    "                cypher = cypher_pool[7]\n",
    "                session.run(cypher, name_val=name_val\n",
    "                                , block_idx_val=block_idx_val\n",
    "                                , parent_page_idx_val=table_parent_page_idx_val\n",
    "                                , parent_hash_val=table_parent_hash_val\n",
    "                                , parent_block_idx_val=table_parent_block_idx_val\n",
    "                                , doc_url_hash_val=doc_url_hash_val\n",
    "                            )\n",
    "\n",
    "            else:   # link table to Document\n",
    "                cypher = cypher_pool[8]\n",
    "                session.run(cypher, name_val=name_val\n",
    "                                , block_idx_val=block_idx_val\n",
    "                                , doc_url_hash_val=doc_url_hash_val\n",
    "                            )\n",
    "            countTable += 1\n",
    "\n",
    "        # **** for tb in doc.tables():\n",
    "        \n",
    "        print(f'\\'{doc_url_val}\\' Done! Summary: ')\n",
    "        print('#Sections: ' + str(countSection))\n",
    "        print('#Chunks: ' + str(countChunk))\n",
    "        print('#Tables: ' + str(countTable))\n",
    "\n",
    "    driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create constraints and indexes\n",
    "\n",
    "initialiseNeo4j()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#PDF files found: 6!\n",
      "Managing /home/olivi/devt/graph-rag/pdfs/quos-contrat-d-mission-oc.pdf\n",
      "'/home/olivi/devt/graph-rag/pdfs/quos-contrat-d-mission-oc.pdf' Done! Summary: \n",
      "#Sections: 27\n",
      "#Chunks: 85\n",
      "#Tables: 0\n",
      "Managing /home/olivi/devt/graph-rag/pdfs/memo-axioma-2023-sowefund.pdf\n",
      "'/home/olivi/devt/graph-rag/pdfs/memo-axioma-2023-sowefund.pdf' Done! Summary: \n",
      "#Sections: 1\n",
      "#Chunks: 28\n",
      "#Tables: 0\n",
      "Managing /home/olivi/devt/graph-rag/pdfs/mainbot-reporting-mai-2022.pdf\n",
      "'/home/olivi/devt/graph-rag/pdfs/mainbot-reporting-mai-2022.pdf' Done! Summary: \n",
      "#Sections: 4\n",
      "#Chunks: 11\n",
      "#Tables: 4\n",
      "Managing /home/olivi/devt/graph-rag/pdfs/notes-d-explication-pour-les-associ-s-des-holding-sowefund.pdf\n",
      "'/home/olivi/devt/graph-rag/pdfs/notes-d-explication-pour-les-associ-s-des-holding-sowefund.pdf' Done! Summary: \n",
      "#Sections: 0\n",
      "#Chunks: 16\n",
      "#Tables: 0\n",
      "Managing /home/olivi/devt/graph-rag/pdfs/quos-rapport-du-pr-sident-11-04-2023.pdf\n",
      "'/home/olivi/devt/graph-rag/pdfs/quos-rapport-du-pr-sident-11-04-2023.pdf' Done! Summary: \n",
      "#Sections: 9\n",
      "#Chunks: 24\n",
      "#Tables: 0\n",
      "Managing /home/olivi/devt/graph-rag/pdfs/biocellvia-reporting-s1-2022.pdf\n",
      "'/home/olivi/devt/graph-rag/pdfs/biocellvia-reporting-s1-2022.pdf' Done! Summary: \n",
      "#Sections: 5\n",
      "#Chunks: 9\n",
      "#Tables: 0\n",
      "Total time: 0:00:17.717854\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get all documents under the folder\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "pdf_files = glob.glob(file_location + '/*.pdf')\n",
    "\n",
    "print(f'#PDF files found: {len(pdf_files)}!')\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "\n",
    "# parse documents and create graph\n",
    "startTime = datetime.now()\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"Managing {pdf_file}\")\n",
    "    doc = pdf_reader.read_pdf(pdf_file)\n",
    "\n",
    "    # find the first / in pdf_file from right\n",
    "    idx = pdf_file.rfind('/')\n",
    "    pdf_file_name = pdf_file[idx+1:]\n",
    "\n",
    "    # open a local file to write the JSON\n",
    "    with open(pdf_file_name + '.json', 'w') as f:\n",
    "        # convert doc.json from a list to string\n",
    "        f.write(str(doc.json))\n",
    "\n",
    "    ingestDocumentNeo4j(doc, pdf_file)\n",
    "\n",
    "print(f'Total time: {datetime.now() - startTime}')\n",
    "\n",
    "# DONE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
